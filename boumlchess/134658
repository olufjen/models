format 224
"mdp" // aima::core::probability::mdp
  revision 3
  modified_by 2 "olufj"
  // class settings
  
  classdiagramsettings member_max_width 0 end
  
  classcompositediagramsettings end
  
  usecasediagramsettings end
  
  sequencediagramsettings end
  
  collaborationdiagramsettings end
  
  objectdiagramsettings end
  
  objectcompositediagramsettings end
  
  componentdiagramsettings
   end
  
  deploymentdiagramsettings
   end
  
  statediagramsettings
   end
  
  activitydiagramsettings
   end
  
  java_dir "C:\\svnroot\\logic\\ai\\trunk\\games\\src\\main\\java\\aima/core/probability/mdp/"
  java_package "aima.core.probability.mdp"
  package_ref 134786 // impl

  classview 133250 "mdp"
    
    classdiagramsettings member_max_width 0 end
    
    classcompositediagramsettings end
    
    collaborationdiagramsettings end
    
    objectdiagramsettings end
    
    objectcompositediagramsettings end
    
    sequencediagramsettings end
    
    statediagramsettings
     end
    
    
    activitydiagramsettings
     end
    class 154370 "PolicyEvaluation"
      visibility public stereotype "interface"
      nformals 4
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}interface ${name}${extends} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "Artificial Intelligence A Modern Approach (3rd Edition): page 656.<br>
<br>
Given a policy &pi;<sub>i</sub>, calculate
U<sub>i</sub>=U<sup>&pi;<sub>i</sub></sup>, the utility of each state if
&pi;<sub>i</sub> were to be executed.

@param <S>
           the state type.
@param <A>
           the action type.

@author Ciaran O'Reilly
@author Ravi Mohan
"
      operation 424578 "evaluate"
	cpp_inline public explicit_return_type "Map<S, Double>"
	nparams 3
	  param inout name "pi_i" explicit_type "Map<S, A>"
	  param inout name "U" explicit_type "Map<S, Double>"
	  param inout name "mdp" type class_ref 183298 // MarkovDecisionProcess
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}, ${t1} ${p1}, ${t2}<S, A> ${p2}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "<b>Policy evaluation:</b> given a policy &pi;<sub>i</sub>, calculate
U<sub>i</sub>=U<sup>&pi;<sub>i</sub></sup>, the utility of each state if
&pi;<sub>i</sub> were to be executed.

@param pi_i
           a policy vector indexed by state
@param U
           a vector of utilities for states in S
@param mdp
           an MDP with states S, actions A(s), transition model P(s'|s,a)
@return U<sub>i</sub>=U<sup>&pi;<sub>i</sub></sup>, the utility of each
        state if &pi;<sub>i</sub> were to be executed.
"
      end

      operation 2424834 "evaluate"
	cpp_inline public explicit_return_type "Map<S, Double>"
	nparams 3
	  param inout name "pi_i" explicit_type "Map<S, A>"
	  param inout name "U" explicit_type "Map<S, Double>"
	  param inout name "mdp" type class_ref 183298 // MarkovDecisionProcess
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}, ${t1} ${p1}, ${t2}<S, A> ${p2}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "<b>Policy evaluation:</b> given a policy &pi;<sub>i</sub>, calculate
U<sub>i</sub>=U<sup>&pi;<sub>i</sub></sup>, the utility of each state if
&pi;<sub>i</sub> were to be executed.

@param pi_i
           a policy vector indexed by state
@param U
           a vector of utilities for states in S
@param mdp
           an MDP with states S, actions A(s), transition model P(s'|s,a)
@return U<sub>i</sub>=U<sup>&pi;<sub>i</sub></sup>, the utility of each
        state if &pi;<sub>i</sub> were to be executed.
"
      end
    end

    class 154626 "ActionsFunction"
      visibility public stereotype "interface"
      nformals 4
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}interface ${name}${extends} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "An interface for MDP action functions.

@param <S>
           the state type.
@param <A>
           the action type.

@author Ciaran O'Reilly
@author Ravi Mohan
"
      operation 423682 "actions"
	cpp_inline public explicit_return_type "Set<A>"
	nparams 1
	  param inout name "s" explicit_type "S"
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the set of actions for state s.

@param s
           the state.
@return the set of actions for state s.
"
      end

      operation 2423938 "actions"
	cpp_inline public explicit_return_type "Set<A>"
	nparams 1
	  param inout name "s" explicit_type "S"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the set of actions for state s.

@param s
           the state.
@return the set of actions for state s.
"
      end
    end

    class 155138 "TransitionProbabilityFunction"
      visibility public stereotype "interface"
      nformals 4
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}interface ${name}${extends} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "An interface for MDP transition probability functions.

@param <S>
           the state type.
@param <A>
           the action type.

@author Ciaran O'Reilly
@author Ravi Mohan
"
      operation 425090 "probability"
	cpp_inline public explicit_return_type "double"
	nparams 3
	  param inout name "sDelta" explicit_type "S"
	  param inout name "s" explicit_type "S"
	  param inout name "a" explicit_type "A"
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}, ${t1} ${p1}, ${t2} ${p2}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Return the probability of going from state s using action a to s' based
on the underlying transition model P(s' | s, a).

@param sDelta
           the state s' being transitioned to.
@param s
           the state s being transitions from.
@param a
           the action used to move from state s to s'.
@return the probability of going from state s using action a to s'.
"
      end

      operation 2424962 "probability"
	cpp_inline public explicit_return_type "double"
	nparams 3
	  param inout name "sDelta" explicit_type "S"
	  param inout name "s" explicit_type "S"
	  param inout name "a" explicit_type "A"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}, ${t1} ${p1}, ${t2} ${p2}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Return the probability of going from state s using action a to s' based
on the underlying transition model P(s' | s, a).

@param sDelta
           the state s' being transitioned to.
@param s
           the state s being transitions from.
@param a
           the action used to move from state s to s'.
@return the probability of going from state s using action a to s'.
"
      end
    end

    class 183298 "MarkovDecisionProcess"
      visibility public stereotype "interface"
      nformals 4
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}interface ${name}${extends} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "Artificial Intelligence A Modern Approach (3rd Edition): page 647.<br>
<br>

A sequential decision problem for a fully observable, stochastic environment
with a Markovian transition model and additive rewards is called a <b>Markov
decision process</b>, or <b>MDP</b>, and consists of a set of states (with an
initial state s<sub>0</sub>; a set ACTIONS(s) of actions in each state; a
transition model P(s' | s, a); and a reward function R(s).<br>
<br>
<b>Note:</b> Some definitions of MDPs allow the reward to depend on the
action and outcome too, so the reward function is R(s, a, s'). This
simplifies the description of some environments but does not change the
problem in any fundamental way.

@param <S>
           the state type.
@param <A>
           the action type.

@author Ciaran O'Reilly
@author Ravi Mohan

"
      operation 423810 "states"
	cpp_inline public explicit_return_type "Set<S>"
	nparams 0
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the set of states associated with the Markov decision process.

@return the set of states associated with the Markov decision process.
"
      end

      operation 423938 "getInitialState"
	cpp_inline public explicit_return_type "S"
	nparams 0
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the initial state s<sub>0</sub> for this instance of a Markov
decision process.

@return the initial state s<sub>0</sub>.
"
      end

      operation 424066 "actions"
	cpp_inline public explicit_return_type "Set<A>"
	nparams 1
	  param inout name "s" explicit_type "S"
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the set of actions for state s.

@param s
           the state.
@return the set of actions for state s.
"
      end

      operation 424194 "transitionProbability"
	cpp_inline public explicit_return_type "double"
	nparams 3
	  param inout name "sDelta" explicit_type "S"
	  param inout name "s" explicit_type "S"
	  param inout name "a" explicit_type "A"
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}, ${t1} ${p1}, ${t2} ${p2}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Return the probability of going from state s using action a to s' based
on the underlying transition model P(s' | s, a).

@param sDelta
           the state s' being transitioned to.
@param s
           the state s being transitions from.
@param a
           the action used to move from state s to s'.
@return the probability of going from state s using action a to s'.
"
      end

      operation 424322 "reward"
	cpp_inline public explicit_return_type "double"
	nparams 1
	  param inout name "s" explicit_type "S"
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the reward associated with being in state s.

@param s
           the state whose award is sought.
@return the reward associated with being in state s.
"
      end

      operation 2424066 "states"
	cpp_inline public explicit_return_type "Set<S>"
	nparams 0
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the set of states associated with the Markov decision process.

@return the set of states associated with the Markov decision process.
"
      end

      operation 2424194 "getInitialState"
	cpp_inline public explicit_return_type "S"
	nparams 0
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the initial state s<sub>0</sub> for this instance of a Markov
decision process.

@return the initial state s<sub>0</sub>.
"
      end

      operation 2424322 "actions"
	cpp_inline public explicit_return_type "Set<A>"
	nparams 1
	  param inout name "s" explicit_type "S"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the set of actions for state s.

@param s
           the state.
@return the set of actions for state s.
"
      end

      operation 2424450 "transitionProbability"
	cpp_inline public explicit_return_type "double"
	nparams 3
	  param inout name "sDelta" explicit_type "S"
	  param inout name "s" explicit_type "S"
	  param inout name "a" explicit_type "A"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}, ${t1} ${p1}, ${t2} ${p2}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Return the probability of going from state s using action a to s' based
on the underlying transition model P(s' | s, a).

@param sDelta
           the state s' being transitioned to.
@param s
           the state s being transitions from.
@param a
           the action used to move from state s to s'.
@return the probability of going from state s using action a to s'.
"
      end

      operation 2424578 "reward"
	cpp_inline public explicit_return_type "double"
	nparams 1
	  param inout name "s" explicit_type "S"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Get the reward associated with being in state s.

@param s
           the state whose award is sought.
@return the reward associated with being in state s.
"
      end
    end

    class 183426 "RewardFunction"
      visibility public stereotype "interface"
      nformals 2
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}interface ${name}${extends} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "An interface for MDP reward functions.

@param <S> The type used to represent states.

@author Ciaran O'Reilly
@author Ravi Mohan
"
      classrelation 216450 // <realization>
	relation 216450 -_-|>
	  stereotype "bind"
	  a public
	    java "${type}<S, Double>"
	    classrelation_ref 216450 // <realization>
	  b parent class_ref 138370 // Function
      end
    end

    class 184450 "Policy"
      visibility public stereotype "interface"
      nformals 4
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}interface ${name}${extends} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "Artificial Intelligence A Modern Approach (3rd Edition): page 647.<br>
<br>

A solution to a Markov decision process is called a <b>policy</b>. It
specifies what the agent should do for any state that the agent might reach.
It is traditional to denote a policy by &pi;, and &pi;(s) is the action
recommended by the policy &pi; for state s. If the agent has a complete
policy, then no matter what the outcome of any action, the agent will always
know what to do next.

@param <S>
           the state type.
@param <A>
           the action type.

@author Ciaran O'Reilly
@author Ravi Mohan

"
      operation 424450 "action"
	cpp_inline public explicit_return_type "A"
	nparams 1
	  param inout name "s" explicit_type "S"
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "&pi;(s) is the action recommended by the policy &pi; for state s.

@param s
           the state s
@return the action recommended by the policy &pi; for state s.
"
      end

      operation 2424706 "action"
	cpp_inline public explicit_return_type "A"
	nparams 1
	  param inout name "s" explicit_type "S"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "&pi;(s) is the action recommended by the policy &pi; for state s.

@param s
           the state s
@return the action recommended by the policy &pi; for state s.
"
      end
    end

    class 184578 "POMDP"
      visibility public stereotype "interface"
      nformals 4
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      nactuals 4
      actual class class_ref 183298 // MarkovDecisionProcess
        rank 0 explicit_value "S"
      actual class class_ref 183298 // MarkovDecisionProcess
        rank 1 explicit_value "A"
      actual class class_ref 183298 // MarkovDecisionProcess
        rank 2 explicit_value ""
      actual class class_ref 183298 // MarkovDecisionProcess
        rank 3 explicit_value ""
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}interface ${name}${extends} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "Artificial Intelligence A Modern Approach (3rd Edition): page 658.<br>
<br>
<p>
<p>
A POMDP has the same elements as an MDP—the transition model P (s' | s, a), actions A(s), and reward function
R(s)—but, like the partially observable search problems, it also has a sensor
model P (e | s).

@param <S> the state type.
@param <A> the action type.
@author samagra
"
      classrelation 216322 // <realization>
	relation 216322 -_-|>
	  stereotype "bind"
	  a public
	    java "${type}"
	    classrelation_ref 216322 // <realization>
	  b parent class_ref 183298 // MarkovDecisionProcess
      end

      operation 424706 "getDiscount"
	cpp_inline public explicit_return_type "double"
	nparams 0
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "@return The discount on future rewards i.e gamma.
"
      end

      operation 424834 "sensorModel"
	cpp_inline public explicit_return_type "double"
	nparams 2
	  param inout name "observedState" explicit_type "S"
	  param inout name "actualState" explicit_type "S"
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}, ${t1} ${p1}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Return the probability of observing a state e while being present in the state s based
on the underlying sendor model P(e | s).

@param observedState the observed state e.
@param actualState   the state s in which the agent is actually present.
@return the probability of observing state e while being in state s.
"
      end

      operation 424962 "getAllActions"
	cpp_inline public explicit_return_type "Set<A>"
	nparams 0
	
	java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "@return All the actions available in the environment.
"
      end
    end
  end

  deploymentview 135810 "mdp"
    //deployment diagram settings
    deploymentdiagramsettings
     end
    artifact 174210 "ActionsFunction"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import java.util.Set;
import aima.core.agent.Action;
${definition}"
      associated_elems
	class_ref 154626 // ActionsFunction
      end
    end

    artifact 174338 "MarkovDecisionProcess"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import java.util.Set;
import aima.core.agent.Action;
${definition}"
      associated_elems
	class_ref 183298 // MarkovDecisionProcess
      end
    end

    artifact 174466 "Policy"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import aima.core.agent.Action;
${definition}"
      associated_elems
	class_ref 184450 // Policy
      end
    end

    artifact 174594 "PolicyEvaluation"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import java.util.Map;
import aima.core.agent.Action;
${definition}"
      associated_elems
	class_ref 154370 // PolicyEvaluation
      end
    end

    artifact 174722 "POMDP"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import aima.core.agent.Action;
import java.util.Set;
${definition}"
      associated_elems
	class_ref 184578 // POMDP
      end
    end

    artifact 174850 "RewardFunction"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import java.util.function.Function;
${definition}"
      associated_elems
	class_ref 183426 // RewardFunction
      end
    end

    artifact 174978 "TransitionProbabilityFunction"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import aima.core.agent.Action;
${definition}"
      associated_elems
	class_ref 155138 // TransitionProbabilityFunction
      end
    end
  end

  package_ref 138114 // search
end

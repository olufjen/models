format 223
"search" // aima::core::probability::mdp::search
  revision 1
  modified_by 2 "bruker"
  // class settings
  
  classdiagramsettings member_max_width 0 end
  
  classcompositediagramsettings end
  
  usecasediagramsettings end
  
  sequencediagramsettings end
  
  collaborationdiagramsettings end
  
  objectdiagramsettings end
  
  objectcompositediagramsettings end
  
  componentdiagramsettings
   end
  
  deploymentdiagramsettings
   end
  
  statediagramsettings
   end
  
  activitydiagramsettings
   end
  
  java_dir "C:\\svnroot\\logic\\ai\\trunk\\games\\src\\main\\java\\aima/core/probability/mdp/search/"
  java_package "aima.core.probability.mdp.search"
  classview 137090 "search"
    
    classdiagramsettings member_max_width 0 end
    
    classcompositediagramsettings end
    
    collaborationdiagramsettings end
    
    objectdiagramsettings end
    
    objectcompositediagramsettings end
    
    sequencediagramsettings end
    
    statediagramsettings
     end
    
    
    activitydiagramsettings
     end
    class 184962 "PolicyIteration"
      visibility public 
      nformals 2
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}${final}${abstract}class ${name}${extends}${implements} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "Artificial Intelligence A Modern Approach (3rd Edition): page 657.<br>
<br>

<pre>
function POLICY-ITERATION(mdp) returns a policy
  inputs: mdp, an MDP with states S, actions A(s), transition model P(s' | s, a)
  local variables: U, a vector of utilities for states in S, initially zero
                   &pi;, a policy vector indexed by state, initially random
                   
  repeat
     U <- POLICY-EVALUATION(&pi;, U, mdp)
     unchanged? <- true
     for each state s in S do
         if max<sub>a &isin; A(s)</sub> &Sigma;<sub>s'</sub>P(s'|s,a)U[s'] > &Sigma;<sub>s'</sub>P(s'|s,&pi;[s])U[s'] then do
            &pi;[s] <- argmax<sub>a &isin; A(s)</sub> &Sigma;<sub>s'</sub>P(s'|s,a)U[s']
            unchanged? <- false
  until unchanged?
  return &pi;
</pre>

Figure 17.7 The policy iteration algorithm for calculating an optimal policy.

@param <S>
           the state type.
@param <A>
           the action type.

@author Ciaran O'Reilly
@author Ravi Mohan

"
      classrelation 217346 // policyEvaluation (<unidirectional association>)
	relation 217346 --->
	  a role_name "policyEvaluation" init_value " null" private
	    java "  ${comment}${@}${visibility}${static}${final}${transient}${volatile}${type}<S, A> ${name}${value};
"
	    classrelation_ref 217346 // policyEvaluation (<unidirectional association>)
	  b parent class_ref 154370 // PolicyEvaluation
      end

      operation 426498 "PolicyIteration"
	cpp_inline public explicit_return_type ""
	nparams 1
	  param inout name "policyEvaluation" type class_ref 154370 // PolicyEvaluation
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${synchronized}${name}${(}${t0}<S, A> ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Constructor.

@param policyEvaluation
           the policy evaluation function to use.
"
      end

      operation 426626 "policyIteration"
	cpp_inline public return_type class_ref 184450 // Policy
	nparams 1
	  param inout name "mdp" type class_ref 183298 // MarkovDecisionProcess
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type}<S, A> ${name}${(}${t0}<S, A> ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment " function POLICY-ITERATION(mdp) returns a policy

The policy iteration algorithm for calculating an optimal policy.

@param mdp
           an MDP with states S, actions A(s), transition model P(s'|s,a)
@return an optimal policy
"
      end

      operation 426754 "initialPolicyVector"
	class_operation cpp_inline public explicit_return_type "Map<S, A>"
	nparams 1
	  param inout name "mdp" type class_ref 183298 // MarkovDecisionProcess
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}<S, A extends Action> ${type} ${name}${(}${t0}<S, A> ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Create a policy vector indexed by state, initially random.

@param mdp
           an MDP with states S, actions A(s), transition model P(s'|s,a)
@return a policy vector indexed by state, initially random.
"
      end
    end

    class 185090 "POMDPValueIteration"
      visibility public 
      nformals 2
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}${final}${abstract}class ${name}${extends}${implements} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "Artificial Intelligence A Modern Approach (3rd Edition): page 663.<br>
<br>

<pre>
function POMDP-VALUE-ITERATION(pomdp, ε) returns a utility function
 inputs: pomdp, a POMDP with states S, actions A(s), transition model P(s′ | s, a),
      sensor model P(e | s), rewards R(s), discount γ
     ε, the maximum error allowed in the utility of any state
 local variables: U, U′, sets of plans p with associated utility vectors αp

 U′  a set containing just the empty plan [], with α[](s) = R(s)
 repeat
   U  U′
   U′  the set of all plans consisting of an action and, for each possible next percept,
     a plan in U with utility vectors computed according to Equation(17.13)
   U′  REMOVE-DOMINATED-PLANS(U′)
 until MAX-DIFFERENCE OLJ THis comment has been changed
 return U
</pre>
<p>
Figure 17.9 A high-level sketch of the value iteration algorithm for POMDPs.
The REMOVE-DOMINATED-PLANS step and MAX-DIFFERENCE test are
typically implemented as linear programs.

@param <S> the state type.
@param <A> the action type.
@author samagra
"
      classrelation 217474 // pomdp (<unidirectional association>)
	relation 217474 --->
	  a role_name "pomdp" public
	    java "  ${comment}${@}${visibility}${static}${final}${transient}${volatile}${type}<S, A> ${name}${value};
"
	    classrelation_ref 217474 // pomdp (<unidirectional association>)
	  b parent class_ref 184578 // POMDP
      end

      attribute 183554 "maxError"
	public explicit_type "double"
	cpp_decl ""
	java_decl "  ${comment}${@}${visibility}${static}${final}${transient}${volatile}${type} ${name}${value};
"
	php_decl ""
	python_decl ""
	idl_decl ""
	mysql_decl ""
	MysqlColumn
      end

      attribute 183682 "depth"
	public explicit_type "int"
	cpp_decl ""
	java_decl "  ${comment}${@}${visibility}${static}${final}${transient}${volatile}${type} ${name}${value};
"
	php_decl ""
	python_decl ""
	idl_decl ""
	mysql_decl ""
	MysqlColumn
      end

      operation 426882 "POMDPValueIteration"
	cpp_inline public explicit_return_type ""
	nparams 3
	  param inout name "pomdp" type class_ref 184578 // POMDP
	  param in name "maxError" explicit_type "double"
	  param in name "maxDepth" explicit_type "int"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${synchronized}${name}${(}${t0}<S, A> ${p0}, ${t1} ${p1}, ${t2} ${p2}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment " function POMDP-VALUE-ITERATION(pomdp, ε) returns a utility function
 inputs: pomdp, a POMDP with states S, actions A(s), transition model P(s′ | s, a),
                  sensor model P(e | s), rewards R(s), discount γ
      ε, the maximum error allowed in the utility of any state"
      end

      operation 427010 "pomdpValueIteration"
	cpp_inline public explicit_return_type "HashMap<List<A>, List<Double>>"
	nparams 0
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "The pomdp-Value-Iteration algorithm

@return returns a utility function
"
      end

      operation 427138 "increasePlanDepths"
	cpp_inline private explicit_return_type "HashMap<List<A>, List<Double>>"
	nparams 1
	  param inout name "uDash" explicit_type "HashMap<List<A>,
            List<Double>>"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "the set of all plans consisting of an action and, for each possible next percept,
a plan in U with utility vectors computed according to Equation(17.13)

@param uDash The utility function of depth d-1
@return The utility function of depth D.
"
      end

      operation 427266 "removeDominatedPlans"
	cpp_inline private explicit_return_type "HashMap<List<A>, List<Double>>"
	nparams 1
	  param inout name "uDash" explicit_type "HashMap<List<A>, List<Double>>"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "This method can be overridden to eliminate dominated plans

@param uDash
@return Non dominated plans.
"
      end

      operation 427394 "maxDifference"
	cpp_inline private explicit_return_type "double"
	nparams 2
	  param inout name "u" explicit_type "HashMap<List<A>, List<Double>>"
	  param inout name "uDash" explicit_type "HashMap<List<A>, List<Double>>"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0} ${p0}, ${t1} ${p1}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "This method can be overridden to apply a different difference calculation function.
It currently calculates the difference of the average of the maximum utilities of the plans

@param u     First utility functtion
@param uDash Second Utility Function.
@return The maximum difference.
"
      end
    end

    class 185218 "ValueIteration"
      visibility public 
      nformals 2
      formal name "S" type "" explicit_default_value ""
        explicit_extends ""
      formal name "A" type "" explicit_default_value ""
        extends class_ref 128002 // Action
      cpp_decl ""
      java_decl "${comment}${@}${visibility}${static}${final}${abstract}class ${name}${extends}${implements} {
${members}}
"
      php_decl ""
      python_2_2 python_decl ""
      idl_decl ""
      explicit_switch_type ""
      mysql_decl ""
      
      comment "Artificial Intelligence A Modern Approach (3rd Edition): page 653.<br>
<br>

<pre>
function VALUE-ITERATION(mdp, &epsilon;) returns a utility function
  inputs: mdp, an MDP with states S, actions A(s), transition model P(s' | s, a),
            rewards R(s), discount &gamma;
          &epsilon; the maximum error allowed in the utility of any state
  local variables: U, U', vectors of utilities for states in S, initially zero
                   &delta; the maximum change in the utility of any state in an iteration
                   
  repeat
      U <- U'; &delta; <- 0
      for each state s in S do
          U'[s] <- R(s) + &gamma;  max<sub>a &isin; A(s)</sub> &Sigma;<sub>s'</sub>P(s' | s, a) U[s']
          if |U'[s] - U[s]| > &delta; then &delta; <- |U'[s] - U[s]|
  until &delta; < &epsilon;(1 - &gamma;)/&gamma;
  return U
</pre>

Figure 17.4 The value iteration algorithm for calculating utilities of
states. The termination condition is from Equation (17.8):<br>

<pre>
if ||U<sub>i+1</sub> - U<sub>i</sub>|| < &epsilon;(1 - &gamma;)/&gamma; then ||U<sub>i+1</sub> - U|| < &epsilon;
</pre>

@param <S>
           the state type.
@param <A>
           the action type.

@author Ciaran O'Reilly
@author Ravi Mohan

"
      attribute 183810 "gamma"
	private explicit_type "double"
	init_value " 0"
	cpp_decl ""
	java_decl "  ${comment}${@}${visibility}${static}${final}${transient}${volatile}${type} ${name}${value};
"
	php_decl ""
	python_decl ""
	idl_decl ""
	mysql_decl ""
	MysqlColumn
	comment " discount &gamma; to be used.
"
      end

      operation 427522 "ValueIteration"
	cpp_inline public explicit_return_type ""
	nparams 1
	  param in name "gamma" explicit_type "double"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${synchronized}${name}${(}${t0} ${p0}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment "Constructor.

@param gamma
           discount &gamma; to be used.
"
      end

      operation 427650 "valueIteration"
	cpp_inline public explicit_return_type "Map<S, Double>"
	nparams 2
	  param inout name "mdp" type class_ref 183298 // MarkovDecisionProcess
	  param in name "epsilon" explicit_type "double"
	
	preserve_java_body_indent java_def "  ${comment}${@}${visibility}${final}${static}${abstract}${synchronized}${type} ${name}${(}${t0}<S, A> ${p0}, ${t1} ${p1}${)}${throws}${staticnl}{
  ${body}}
"
	
	
	
	comment " function VALUE-ITERATION(mdp, &epsilon;) returns a utility function

The value iteration algorithm for calculating the utility of states.

@param mdp
           an MDP with states S, actions A(s), <br>
           transition model P(s' | s, a), rewards R(s)
@param epsilon
           the maximum error allowed in the utility of any state
@return a vector of utilities for states in S
"
      end
    end
  end

  deploymentview 136066 "search"
    //deployment diagram settings
    deploymentdiagramsettings
     end
    artifact 175490 "PolicyIteration"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import java.util.ArrayList;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import aima.core.agent.Action;
import aima.core.probability.mdp.MarkovDecisionProcess;
import aima.core.probability.mdp.Policy;
import aima.core.probability.mdp.PolicyEvaluation;
import aima.core.probability.mdp.impl.LookupPolicy;
import aima.core.util.Util;
${definition}"
      associated_elems
	class_ref 184962 // PolicyIteration
      end
    end

    artifact 175618 "POMDPValueIteration"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import aima.core.agent.Action;
import aima.core.probability.mdp.POMDP;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
${definition}"
      associated_elems
	class_ref 185090 // POMDPValueIteration
      end
    end

    artifact 175746 "ValueIteration"
      stereotype "source"
      java_src "${comment}
${package}
${imports}
import java.util.Map;
import java.util.Set;
import aima.core.agent.Action;
import aima.core.probability.mdp.MarkovDecisionProcess;
import aima.core.util.Util;
${definition}"
      associated_elems
	class_ref 185218 // ValueIteration
      end
    end
  end
end
